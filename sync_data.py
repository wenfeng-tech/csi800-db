import os
import sys
import logging
import argparse
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import akshare as ak
import pandas as pd
from supabase import create_client, Client

# --- é…ç½®åŒºåŸŸ ---

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– Supabase é…ç½®
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# è®¾ç½®å¹¶å‘ä¸‹è½½çš„çº¿ç¨‹æ•°
MAX_WORKERS = 10
# è®¾ç½®æ•°æ®åº“åˆ†æ‰¹æ’å…¥çš„å¤§å°
BATCH_SIZE = 50

# --- æ•°æ®è·å–ä¸å¤„ç†å‡½æ•° ---

def get_csi800_stock_info() -> dict:
    """
    è·å–æœ€æ–°çš„ä¸­è¯800æˆåˆ†è‚¡ä»£ç å’Œå¯¹åº”çš„å…¬å¸åç§°
    """
    try:
        stock_df = ak.index_stock_cons_csindex(symbol="000906")
        logging.info(f"æˆåŠŸä»ä¸­è¯æŒ‡æ•°å®˜ç½‘è·å–ä¸­è¯800æˆåˆ†è‚¡ï¼Œå…± {len(stock_df)} åªè‚¡ç¥¨ã€‚")
        return pd.Series(stock_df['æˆåˆ†åˆ¸åç§°'].values, index=stock_df['æˆåˆ†åˆ¸ä»£ç ']).to_dict()
    except Exception as e:
        logging.error(f"é”™è¯¯ï¼šè·å–ä¸­è¯800æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥ - {e}")
        return {}


def get_stock_history(stock_code: str, stock_name: str, start_date: str) -> pd.DataFrame:
    """
    è·å–å•åªè‚¡ç¥¨å†å²æ•°æ®ã€‚ä¸åŒ…å«å†…éƒ¨é‡è¯•é€»è¾‘ã€‚
    """
    try:
        stock_hist_df = ak.stock_zh_a_hist(symbol=stock_code, period="daily", start_date=start_date, adjust="qfq")

        if not stock_hist_df.empty:
            stock_hist_df.rename(columns={
                'æ—¥æœŸ': 'trade_date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 'æœ€é«˜': 'high',
                'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume'
            }, inplace=True)
            stock_hist_df['stock_code'] = stock_code
            stock_hist_df['stock_name'] = stock_name
            stock_hist_df['trade_date'] = pd.to_datetime(stock_hist_df['trade_date']).dt.strftime('%Y-%m-%d')
            
            required_columns = [
                'trade_date', 'stock_code', 'stock_name',
                'open', 'high', 'low', 'close', 'volume'
            ]
            return stock_hist_df[required_columns]
            
    except Exception as e:
        logging.warning(f"è·å–è‚¡ç¥¨ {stock_code} ({stock_name}) æ•°æ®å¤±è´¥ - {e}")
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿ä¸Šå±‚çŸ¥é“å¤±è´¥äº†

    return pd.DataFrame()


def execute_batch_upsert(supabase_client: Client, data_frames: list) -> int:
    """è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œæ‰¹é‡æ’å…¥å¹¶è¿”å›æ’å…¥çš„è®°å½•æ•°"""
    if not data_frames:
        return 0
        
    full_df = pd.concat(data_frames, ignore_index=True)
    data_to_insert = full_df.to_dict(orient='records')
    record_count = len(data_to_insert)
    
    logging.info(f"å‡†å¤‡æ‰¹é‡æ’å…¥ {record_count} æ¡æ•°æ® (æ‰¹æ¬¡)...")
    try:
        supabase_client.table("csi800_daily_data").upsert(data_to_insert).execute()
        logging.info(f"âœ… æˆåŠŸåŒæ­¥æ‰¹æ¬¡ï¼Œå…± {record_count} æ¡è®°å½•ã€‚")
        return record_count
    except Exception as e:
        logging.error(f"æ•°æ®åº“é”™è¯¯ï¼šæ‰¹æ¬¡æ’å…¥æ•°æ®å¤±è´¥ - {e}")
        return 0


def fetch_and_insert_stocks(supabase_client: Client, stock_info: dict, start_date: str, task_desc: str):
    """
    ã€é€šç”¨æ¨¡å¼ã€‘ä½¿ç”¨å¹¶å‘æŠ€æœ¯è·å–è‚¡ç¥¨æ•°æ®ï¼Œå¹¶åˆ†æ‰¹æ’å…¥æ•°æ®åº“ã€‚
    """
    logging.info(f"å¼€å§‹æ‰§è¡Œ '{task_desc}' ä»»åŠ¡ï¼Œç›®æ ‡è‚¡ç¥¨æ•°: {len(stock_info)}ï¼Œèµ·å§‹æ—¥æœŸ: {start_date}")
    
    batch_data_frames = []
    total_inserted_records = 0
    total_stocks = len(stock_info)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(get_stock_history, code, name, start_date): (code, name) 
                   for code, name in stock_info.items()}

        for i, future in enumerate(as_completed(futures)):
            code, name = futures[future]
            try:
                df = future.result()
                if not df.empty:
                    batch_data_frames.append(df)
                    logging.info(f"è¿›åº¦: {i + 1}/{total_stocks} | æˆåŠŸè·å– {code} ({name}) çš„ {len(df)} æ¡æ•°æ®ã€‚")
            except Exception as e:
                logging.error(f"è¿›åº¦: {i + 1}/{total_stocks} | å¤„ç†è‚¡ç¥¨ {code} ({name}) æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

            # åˆ†æ‰¹å¤„ç†é€»è¾‘
            if len(batch_data_frames) >= BATCH_SIZE or (i + 1) == total_stocks:
                inserted_count = execute_batch_upsert(supabase_client, batch_data_frames)
                total_inserted_records += inserted_count
                batch_data_frames = [] # æ¸…ç©ºæ‰¹æ¬¡

    logging.info(f"ğŸ‰ '{task_desc}' ä»»åŠ¡å®Œæˆï¼æ€»å…±æˆåŠŸæ’å…¥ {total_inserted_records} æ¡è®°å½•ã€‚")


def verify_and_retry_sync(supabase_client: Client, target_stocks: dict):
    """ã€æ ¡éªŒä¿®å¤æ¨¡å¼ã€‘æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ï¼Œå¹¶ä¸ºç¼ºå¤±è‚¡ç¥¨è¿›è¡Œç²¾ç¡®ä¿®å¤ã€‚"""
    logging.info("å¼€å§‹æ‰§è¡Œæ•°æ®æ ¡éªŒä¸ä¿®å¤ (ç²¾ç¡®æ¨¡å¼)...")
    
    if not target_stocks:
        logging.warning("ç›®æ ‡è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œæ ¡éªŒä»»åŠ¡ç»ˆæ­¢ã€‚")
        return
    
    try:
        response = supabase_client.table("csi800_daily_data").select("stock_code, trade_date").execute()
        db_data = pd.DataFrame(response.data)
        db_summary = {}
        if not db_data.empty:
            db_summary = db_data.groupby('stock_code')['trade_date'].max().to_dict()
    except Exception as e:
        logging.error(f"ä»Supabaseè·å–æ•°æ®çŠ¶æ€å¤±è´¥: {e}ã€‚æ— æ³•æ‰§è¡Œç²¾ç¡®ä¿®å¤ã€‚")
        return

    retry_stock_info = {}
    retry_start_dates = {}
    latest_trading_day_threshold = (datetime.now() - timedelta(days=4)).strftime('%Y-%m-%d')
    fallback_start_date = (datetime.now() - timedelta(days=365)).strftime('%Y%m%d')

    for code, name in target_stocks.items():
        start_date_for_fetch = None
        if code in db_summary:
            last_date_str = db_summary[code]
            if last_date_str < latest_trading_day_threshold:
                start_date_obj = datetime.strptime(last_date_str, '%Y-%m-%d') + timedelta(days=1)
                start_date_for_fetch = start_date_obj.strftime('%Y%m%d')
                logging.info(f"å‘ç°è½åè‚¡ç¥¨: {code} ({name}), æœ€æ–°æ—¥æœŸ: {last_date_str}ã€‚å°†ä» {start_date_for_fetch} å¼€å§‹åŒæ­¥ã€‚")
        else:
            start_date_for_fetch = fallback_start_date
            logging.info(f"å‘ç°ç¼ºå¤±è‚¡ç¥¨: {code} ({name})ã€‚å°†ä» {fallback_start_date} å¼€å§‹åŒæ­¥ã€‚")
        
        if start_date_for_fetch:
            retry_stock_info[code] = name
            retry_start_dates[code] = start_date_for_fetch

    if not retry_stock_info:
        logging.info("âœ… æ•°æ®æ ¡éªŒå®Œæˆï¼Œæ‰€æœ‰è‚¡ç¥¨æ•°æ®éƒ½æ˜¯æœ€æ–°çš„ã€‚")
        return

    logging.info(f"\nå…±å‘ç° {len(retry_stock_info)} ä¸ªä¿®å¤ä»»åŠ¡ã€‚å¼€å§‹æ‰§è¡Œ...")
    
    batch_data_frames = []
    total_inserted_records = 0
    total_stocks = len(retry_stock_info)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(get_stock_history, code, name, retry_start_dates[code]): (code, name) 
                   for code, name in retry_stock_info.items()}

        for i, future in enumerate(as_completed(futures)):
            code, name = futures[future]
            try:
                df = future.result()
                if not df.empty:
                    batch_data_frames.append(df)
                    logging.info(f"ä¿®å¤è¿›åº¦: {i + 1}/{total_stocks} | æˆåŠŸä¿®å¤ {code} ({name}) çš„ {len(df)} æ¡æ•°æ®ã€‚")
            except Exception as e:
                logging.error(f"ä¿®å¤è¿›åº¦: {i + 1}/{total_stocks} | ä¿®å¤è‚¡ç¥¨ {code} ({name}) æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

            if len(batch_data_frames) >= BATCH_SIZE or (i + 1) == total_stocks:
                inserted_count = execute_batch_upsert(supabase_client, batch_data_frames)
                total_inserted_records += inserted_count
                batch_data_frames = []

    logging.info(f"ğŸš€ æ ¡éªŒä¸ä¿®å¤ä»»åŠ¡å®Œæˆï¼æ€»å…±æˆåŠŸæ’å…¥ {total_inserted_records} æ¡æ–°è®°å½•ã€‚")


def main():
    """è„šæœ¬ä¸»å…¥å£ï¼Œä½¿ç”¨argparseå¤„ç†å‘½ä»¤è¡Œå‚æ•°"""
    if not SUPABASE_URL or not SUPABASE_KEY:
        logging.error("å…³é”®é…ç½®ç¼ºå¤±ï¼šè¯·è®¾ç½® SUPABASE_URL å’Œ SUPABASE_KEY ç¯å¢ƒå˜é‡ã€‚")
        raise ValueError("Missing Supabase credentials")

    parser = argparse.ArgumentParser(description="ä¸­è¯800è‚¡ç¥¨æ•°æ®åŒæ­¥å·¥å…·")
    parser.add_argument(
        "mode",
        choices=['daily', 'full', 'partial', 'verify'],
        default='daily',
        nargs='?',
        help="é€‰æ‹©åŒæ­¥æ¨¡å¼: 'daily' (è¿‘5å¤©), 'full' (å…¨éƒ¨å†å²), 'partial' (è‡ª2015å¹´), 'verify' (æ ¡éªŒä¿®å¤)."
    )
    args = parser.parse_args()
    sync_mode = args.mode
    
    logging.info(f"--- å½“å‰è¿è¡Œæ¨¡å¼: {sync_mode} ---")
    
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    logging.info("æ­£åœ¨è·å–æœ€æ–°çš„ä¸­è¯800æˆåˆ†è‚¡åˆ—è¡¨...")
    stock_info = get_csi800_stock_info()
    if not stock_info:
        logging.error("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    if sync_mode == 'daily':
        start_date = (datetime.now() - timedelta(days=5)).strftime('%Y%m%d')
        fetch_and_insert_stocks(supabase, stock_info, start_date, "æ¯æ—¥å¢é‡æ›´æ–°")
    
    elif sync_mode == 'verify':
        verify_and_retry_sync(supabase, stock_info)

    elif sync_mode in ['full', 'partial']:
        start_dates = {'full': "20050101", 'partial': "20150101"}
        task_desc = "å…¨é‡å†å²åŒæ­¥" if sync_mode == 'full' else "éƒ¨åˆ†å†å²åŒæ­¥"
        fetch_and_insert_stocks(supabase, stock_info, start_dates[sync_mode], task_desc)


if __name__ == "__main__":
    main()
