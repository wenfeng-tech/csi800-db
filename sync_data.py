import os
import sys
import logging
import argparse
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import akshare as ak
import pandas as pd
from supabase import create_client, Client

# --- é…ç½®åŒºåŸŸ ---

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– Supabase é…ç½®
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# è®¾ç½®å¹¶å‘ä¸‹è½½çš„çº¿ç¨‹æ•°
MAX_WORKERS = 10
# è®¾ç½®æ•°æ®åº“åˆ†æ‰¹æ’å…¥çš„å¤§å°
BATCH_SIZE = 50

# --- æ•°æ®è·å–ä¸å¤„ç†å‡½æ•° ---

def get_csi800_stock_info() -> dict:
    """
    è·å–æœ€æ–°çš„ä¸­è¯800æˆåˆ†è‚¡ä»£ç å’Œå¯¹åº”çš„å…¬å¸åç§°
    """
    try:
        stock_df = ak.index_stock_cons_csindex(symbol="000906")
        logging.info(f"æˆåŠŸä»ä¸­è¯æŒ‡æ•°å®˜ç½‘è·å–ä¸­è¯800æˆåˆ†è‚¡ï¼Œå…± {len(stock_df)} åªè‚¡ç¥¨ã€‚")
        return pd.Series(stock_df['æˆåˆ†åˆ¸åç§°'].values, index=stock_df['æˆåˆ†åˆ¸ä»£ç ']).to_dict()
    except Exception as e:
        logging.error(f"é”™è¯¯ï¼šè·å–ä¸­è¯800æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥ - {e}")
        return {}


def get_stock_history(stock_code: str, stock_name: str, start_date: str) -> pd.DataFrame:
    """
    è·å–å•åªè‚¡ç¥¨å†å²æ•°æ®ã€‚ä¸åŒ…å«å†…éƒ¨é‡è¯•é€»è¾‘ã€‚
    """
    try:
        stock_hist_df = ak.stock_zh_a_hist(symbol=stock_code, period="daily", start_date=start_date, adjust="qfq")

        if not stock_hist_df.empty:
            stock_hist_df.rename(columns={
                'æ—¥æœŸ': 'trade_date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 'æœ€é«˜': 'high',
                'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume'
            }, inplace=True)
            stock_hist_df['stock_code'] = stock_code
            stock_hist_df['stock_name'] = stock_name
            stock_hist_df['trade_date'] = pd.to_datetime(stock_hist_df['trade_date']).dt.strftime('%Y-%m-%d')
            
            required_columns = [
                'trade_date', 'stock_code', 'stock_name',
                'open', 'high', 'low', 'close', 'volume'
            ]
            return stock_hist_df[required_columns]
            
    except Exception as e:
        logging.warning(f"è·å–è‚¡ç¥¨ {stock_code} ({stock_name}) æ•°æ®å¤±è´¥ - {e}")
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿ä¸Šå±‚çŸ¥é“å¤±è´¥äº†

    return pd.DataFrame()


def execute_batch_upsert(supabase_client: Client, data_frames: list) -> int:
    """è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œæ‰¹é‡æ’å…¥å¹¶è¿”å›æ’å…¥çš„è®°å½•æ•°"""
    if not data_frames:
        return 0
        
    full_df = pd.concat(data_frames, ignore_index=True)
    data_to_insert = full_df.to_dict(orient='records')
    record_count = len(data_to_insert)
    
    logging.info(f"å‡†å¤‡æ‰¹é‡æ’å…¥ {record_count} æ¡æ•°æ® (æ‰¹æ¬¡)...")
    try:
        supabase_client.table("csi800_daily_data").upsert(data_to_insert).execute()
        logging.info(f"âœ… æˆåŠŸåŒæ­¥æ‰¹æ¬¡ï¼Œå…± {record_count} æ¡è®°å½•ã€‚")
        return record_count
    except Exception as e:
        logging.error(f"æ•°æ®åº“é”™è¯¯ï¼šæ‰¹æ¬¡æ’å…¥æ•°æ®å¤±è´¥ - {e}")
        return 0


def fetch_and_insert_stocks(supabase_client: Client, stock_info: dict, start_date: str, task_desc: str):
    """
    ã€é€šç”¨æ¨¡å¼ã€‘ä½¿ç”¨å¹¶å‘æŠ€æœ¯è·å–è‚¡ç¥¨æ•°æ®ï¼Œå¹¶åˆ†æ‰¹æ’å…¥æ•°æ®åº“ã€‚
    """
    logging.info(f"å¼€å§‹æ‰§è¡Œ '{task_desc}' ä»»åŠ¡ï¼Œç›®æ ‡è‚¡ç¥¨æ•°: {len(stock_info)}ï¼Œèµ·å§‹æ—¥æœŸ: {start_date}")
    
    batch_data_frames = []
    total_inserted_records = 0
    total_stocks = len(stock_info)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(get_stock_history, code, name, start_date): (code, name) 
                   for code, name in stock_info.items()}

        for i, future in enumerate(as_completed(futures)):
            code, name = futures[future]
            try:
                df = future.result()
                if not df.empty:
                    batch_data_frames.append(df)
                    logging.info(f"è¿›åº¦: {i + 1}/{total_stocks} | æˆåŠŸè·å– {code} ({name}) çš„ {len(df)} æ¡æ•°æ®ã€‚")
            except Exception as e:
                logging.error(f"è¿›åº¦: {i + 1}/{total_stocks} | å¤„ç†è‚¡ç¥¨ {code} ({name}) æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

            # åˆ†æ‰¹å¤„ç†é€»è¾‘
            if len(batch_data_frames) >= BATCH_SIZE or (i + 1) == total_stocks:
                inserted_count = execute_batch_upsert(supabase_client, batch_data_frames)
                total_inserted_records += inserted_count
                batch_data_frames = [] # æ¸…ç©ºæ‰¹æ¬¡

    logging.info(f"ğŸ‰ '{task_desc}' ä»»åŠ¡å®Œæˆï¼æ€»å…±æˆåŠŸæ’å…¥ {total_inserted_records} æ¡è®°å½•ã€‚")


def verify_and_retry_sync(supabase_client: Client, target_stocks: dict):
    """
    ã€æ ¡éªŒä¿®å¤æ¨¡å¼ - ç®€åŒ–ç‰ˆã€‘
    æ£€æŸ¥æ•°æ®åº“ï¼Œæ‰¾å‡ºæ‰€æœ‰æœªè¾¾åˆ°æœ€æ–°äº¤æ˜“æ—¥æœŸçš„è‚¡ç¥¨ï¼Œå¹¶å¯¹å®ƒä»¬ç»Ÿä¸€æ‰§è¡Œä¸€æ¬¡å°è§„æ¨¡çš„æ¯æ—¥æ›´æ–°ã€‚
    """
    logging.info("å¼€å§‹æ‰§è¡Œæ•°æ®æ ¡éªŒä¸ä¿®å¤ (ç®€åŒ–æ¨¡å¼)...")
    
    if not target_stocks:
        logging.warning("ç›®æ ‡è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œæ ¡éªŒä»»åŠ¡ç»ˆæ­¢ã€‚")
        return
    
    try:
        # 1. è·å–æ•°æ®åº“ä¸­æ‰€æœ‰è®°å½•ï¼Œä»¥ç¡®å®šåŸºå‡†æ—¥æœŸ
        response = supabase_client.table("csi800_daily_data").select("stock_code, trade_date").execute()
        db_data = pd.DataFrame(response.data)
        
        if db_data.empty:
            logging.warning("æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œæ ¡éªŒã€‚å»ºè®®å…ˆè¿è¡Œ 'partial' æˆ– 'full' æ¨¡å¼è¿›è¡Œåˆå§‹åŒ–ã€‚")
            # å°†æ‰€æœ‰ç›®æ ‡è‚¡ç¥¨è§†ä¸ºç¼ºå¤±ï¼Œå¹¶è¿›è¡Œä¸€æ¬¡æ¯æ—¥æ›´æ–°
            logging.info("å°†ä¸ºæ‰€æœ‰ç›®æ ‡è‚¡ç¥¨æ‰§è¡Œä¸€æ¬¡æ¯æ—¥å¢é‡åŒæ­¥...")
            start_date_for_retry = (datetime.now() - timedelta(days=5)).strftime('%Y%m%d')
            fetch_and_insert_stocks(supabase_client, target_stocks, start_date_for_retry, "æ•°æ®åº“åˆå§‹åŒ–ä¿®å¤")
            return
            
        # 2. ç¡®å®šå”¯ä¸€çš„æœ€æ–°äº¤æ˜“æ—¥ä½œä¸ºåŸºå‡†
        latest_market_date = db_data['trade_date'].max()
        logging.info(f"æ•°æ®åº“ä¸­çš„æœ€æ–°äº¤æ˜“æ—¥åŸºå‡†ä¸º: {latest_market_date}")

        # 3. æ‰¾å‡ºæ‰€æœ‰è½åæˆ–ç¼ºå¤±çš„è‚¡ç¥¨
        db_summary = db_data.groupby('stock_code')['trade_date'].max()
        
        retry_stock_info = {}
        # æ‰¾å‡ºæ•°æ®è½åçš„è‚¡ç¥¨
        lagging_codes = db_summary[db_summary < latest_market_date].index
        for code in lagging_codes:
            if code in target_stocks:
                retry_stock_info[code] = target_stocks[code]

        # æ‰¾å‡ºå®Œå…¨ç¼ºå¤±çš„è‚¡ç¥¨
        db_codes = set(db_summary.index)
        for code, name in target_stocks.items():
            if code not in db_codes:
                retry_stock_info[code] = name
        
        if not retry_stock_info:
            logging.info(f"âœ… æ•°æ®æ ¡éªŒå®Œæˆï¼Œæ‰€æœ‰è‚¡ç¥¨æ•°æ®éƒ½å·²æ›´æ–°è‡³ {latest_market_date}ã€‚")
            return

        logging.info(f"\nå…±å‘ç° {len(retry_stock_info)} åªè‚¡ç¥¨æœªè¾¾åˆ°æœ€æ–°æ—¥æœŸã€‚å‡†å¤‡è¿›è¡Œä¸€æ¬¡é’ˆå¯¹æ€§çš„æ¯æ—¥æ›´æ–°...")
        
        # 4. å¯¹è¿™äº›è‚¡ç¥¨ç»Ÿä¸€æ‰§è¡Œä¸€æ¬¡â€œæ¯æ—¥æ›´æ–°â€
        start_date_for_retry = (datetime.now() - timedelta(days=5)).strftime('%Y%m%d')
        fetch_and_insert_stocks(
            supabase_client=supabase_client,
            stock_info=retry_stock_info,
            start_date=start_date_for_retry,
            task_desc="æ ¡éªŒä¿®å¤åŒæ­¥"
        )

    except Exception as e:
        logging.error(f"æ‰§è¡Œæ ¡éªŒä¿®å¤æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return


def main():
    """è„šæœ¬ä¸»å…¥å£ï¼Œä½¿ç”¨argparseå¤„ç†å‘½ä»¤è¡Œå‚æ•°"""
    if not SUPABASE_URL or not SUPABASE_KEY:
        logging.error("å…³é”®é…ç½®ç¼ºå¤±ï¼šè¯·è®¾ç½® SUPABASE_URL å’Œ SUPABASE_KEY ç¯å¢ƒå˜é‡ã€‚")
        raise ValueError("Missing Supabase credentials")

    parser = argparse.ArgumentParser(description="ä¸­è¯800è‚¡ç¥¨æ•°æ®åŒæ­¥å·¥å…·")
    parser.add_argument(
        "mode",
        choices=['daily', 'full', 'partial', 'verify'],
        default='daily',
        nargs='?',
        help="é€‰æ‹©åŒæ­¥æ¨¡å¼: 'daily' (è¿‘5å¤©), 'full' (å…¨éƒ¨å†å²), 'partial' (è‡ª2015å¹´), 'verify' (æ ¡éªŒä¿®å¤)."
    )
    args = parser.parse_args()
    sync_mode = args.mode
    
    logging.info(f"--- å½“å‰è¿è¡Œæ¨¡å¼: {sync_mode} ---")
    
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    logging.info("æ­£åœ¨è·å–æœ€æ–°çš„ä¸­è¯800æˆåˆ†è‚¡åˆ—è¡¨...")
    stock_info = get_csi800_stock_info()
    if not stock_info:
        logging.error("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    if sync_mode == 'daily':
        start_date = (datetime.now() - timedelta(days=5)).strftime('%Y%m%d')
        fetch_and_insert_stocks(supabase, stock_info, start_date, "æ¯æ—¥å¢é‡æ›´æ–°")
    
    elif sync_mode == 'verify':
        verify_and_retry_sync(supabase, stock_info)

    elif sync_mode in ['full', 'partial']:
        start_dates = {'full': "20050101", 'partial': "20150101"}
        task_desc = "å…¨é‡å†å²åŒæ­¥" if sync_mode == 'full' else "éƒ¨åˆ†å†å²åŒæ­¥"
        fetch_and_insert_stocks(supabase, stock_info, start_dates[sync_mode], task_desc)


if __name__ == "__main__":
    main()
